{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_Models_LTV.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hanXTEvvY0w8","colab_type":"text"},"source":["### 1. Import data and libraries"]},{"cell_type":"code","metadata":{"id":"L1khXk6tx3e2","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import sys\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","pd.set_option('display.max_columns', None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncH4VCJ3xoAF","colab_type":"code","outputId":"e6af88d4-e105-4045-d9be-502ddabda27a","executionInfo":{"status":"ok","timestamp":1585510267339,"user_tz":420,"elapsed":2610,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["drive.mount('/content/drive',force_remount=True)\n","path =\"/content/drive/My Drive/ML_templates/\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N3iF1Kcm0WYz","colab_type":"code","colab":{}},"source":["sys.path.append('/content/drive/My Drive/ML_templates/utils')\n","import explore_data as ed\n","import evaluate_metrics as em"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-c_E-tIEyfaz","colab_type":"code","outputId":"365a3959-9dc5-4fb1-deec-df4a5fc55dfe","executionInfo":{"status":"ok","timestamp":1585510267909,"user_tz":420,"elapsed":3060,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":266}},"source":["ltv_df = pd.read_csv(path+\"/data/Technical exam data.csv\")\n","print (\"{} dataset has {} rows(samples) with {} columns(features) each.\".format(\"LTV\",*ltv_df.shape))\n","print(ltv_df.head(5))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["LTV dataset has 303235 rows(samples) with 8 columns(features) each.\n","   POLICY_NUMBER START_DATE  START_MONTH CHURN_DATE  MEMBERS  MAX_DURATION  \\\n","0       97662795  01-APR-16            4  01-OCT-16        1           184   \n","1      100915150  01-DEC-16           12  01-DEC-16        1           180   \n","2       99747655  01-OCT-16           10  01-MAR-17        1           180   \n","3       99406855  01-SEP-16            9  01-JAN-17        1           180   \n","4      102166525  01-JAN-17            1  01-FEB-17        2           184   \n","\n","  STATE CARRIER_NAME  \n","0    AZ     CARRIER2  \n","1    OR     CARRIER1  \n","2    NC     CARRIER3  \n","3    AK     CARRIER3  \n","4    AZ     CARRIER2  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bj2Pcctj4Mil","colab_type":"code","outputId":"85cdbb06-20f7-490e-c2fa-ee821b83fe31","executionInfo":{"status":"ok","timestamp":1585510378141,"user_tz":420,"elapsed":113281,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":621}},"source":["self.dataframe=ltv_df\n","ltv_df_filtered[[\"START_DATE\", \"CHURN_DATE\"]] = ltv_df_filtered[[\"START_DATE\", \"CHURN_DATE\"]].apply(pd.to_datetime)\n","ltv_df_filtered[\"START_YEAR\"]=ltv_df_filtered['START_DATE'].dt.year\n","ltv_df_filtered[\"CHURN_YEAR\"]=ltv_df_filtered['CHURN_DATE'].dt.year\n","ltv_df_filtered[\"CHURN_MONTH\"]=ltv_df_filtered['CHURN_DATE'].dt.month\n","ltv_df_filtered[\"MAX_BILLING_CYCLE\"]=round(ltv_df_filtered['MAX_DURATION']/30).astype(int)\n","ltv_df_filtered[\"BILLING_CYCLE_1\"]=ltv_df_filtered['MEMBERS']\n","\n","def billing_cycle(members,cycle,max_billing_cycle):\n","    if cycle <= max_billing_cycle:\n","        return members\n","    else:\n","        return 0\n","\n","ltv_df_filtered['BILLING_CYCLE_2'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,2, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_3'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,3, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_4'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,4, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_5'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,5, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_6'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,6, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_7'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,7, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_8'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,8, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_9'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,9, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_10'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,10, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_11'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,11, x.MAX_BILLING_CYCLE), axis=1)\n","ltv_df_filtered['BILLING_CYCLE_12'] = ltv_df_filtered.apply(lambda x: billing_cycle(x.MEMBERS,12, x.MAX_BILLING_CYCLE), axis=1)\n","\n","print(ltv_df_filtered.head(5))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["   POLICY_NUMBER START_DATE  START_MONTH CHURN_DATE  MEMBERS  MAX_DURATION  \\\n","0       97662795 2016-04-01            4 2016-10-01        1           184   \n","1      100915150 2016-12-01           12 2016-12-01        1           180   \n","2       99747655 2016-10-01           10 2017-03-01        1           180   \n","3       99406855 2016-09-01            9 2017-01-01        1           180   \n","4      102166525 2017-01-01            1 2017-02-01        2           184   \n","\n","  STATE CARRIER_NAME  START_YEAR  CHURN_YEAR  CHURN_MONTH  MAX_BILLING_CYCLE  \\\n","0    AZ     CARRIER2        2016        2016           10                  6   \n","1    OR     CARRIER1        2016        2016           12                  6   \n","2    NC     CARRIER3        2016        2017            3                  6   \n","3    AK     CARRIER3        2016        2017            1                  6   \n","4    AZ     CARRIER2        2017        2017            2                  6   \n","\n","   BILLING_CYCLE_1  BILLING_CYCLE_2  BILLING_CYCLE_3  BILLING_CYCLE_4  \\\n","0                1                1                1                1   \n","1                1                1                1                1   \n","2                1                1                1                1   \n","3                1                1                1                1   \n","4                2                2                2                2   \n","\n","   BILLING_CYCLE_5  BILLING_CYCLE_6  BILLING_CYCLE_7  BILLING_CYCLE_8  \\\n","0                1                1                0                0   \n","1                1                1                0                0   \n","2                1                1                0                0   \n","3                1                1                0                0   \n","4                2                2                0                0   \n","\n","   BILLING_CYCLE_9  BILLING_CYCLE_10  BILLING_CYCLE_11  BILLING_CYCLE_12  \n","0                0                 0                 0                 0  \n","1                0                 0                 0                 0  \n","2                0                 0                 0                 0  \n","3                0                 0                 0                 0  \n","4                0                 0                 0                 0  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A9HZy_FiSrIH","colab_type":"text"},"source":["### 7a. Preliminary Model Selection"]},{"cell_type":"code","metadata":{"id":"ZK8nJTOI5KDc","colab_type":"code","colab":{}},"source":["ltv_df_model=ltv_df_filtered[['POLICY_NUMBER','START_MONTH','START_DATE','MEMBERS','STATE','CARRIER_NAME','MAX_BILLING_CYCLE']]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G00S7Gtw5Mtn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"outputId":"2bf700c8-6f0f-4218-9050-a8339ee115d0","executionInfo":{"status":"ok","timestamp":1585512266360,"user_tz":420,"elapsed":556,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}}},"source":["ltv_df_model.groupby('MAX_BILLING_CYCLE').size()\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MAX_BILLING_CYCLE\n","1         67\n","2          4\n","3      82137\n","4        537\n","6     177990\n","10        40\n","11      7552\n","12     33970\n","36       938\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"N8cKA_3bZm_L","colab_type":"code","outputId":"9b53bc89-39e5-4780-81a9-d37735fb9dae","executionInfo":{"status":"ok","timestamp":1585510378851,"user_tz":420,"elapsed":113937,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":311}},"source":["ltv_df_model=ltv_df_filtered[['POLICY_NUMBER','START_MONTH','START_DATE','MEMBERS','STATE','CARRIER_NAME','MAX_BILLING_CYCLE']]\n","ltv_df_model=ltv_df_model[(ltv_df_model.MAX_BILLING_CYCLE<20) & (ltv_df_model.MAX_BILLING_CYCLE!=2)]\n","ltv_df_model=pd.get_dummies(ltv_df_model)\n","ltv_df_model.describe()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>POLICY_NUMBER</th>\n","      <th>START_MONTH</th>\n","      <th>MEMBERS</th>\n","      <th>MAX_BILLING_CYCLE</th>\n","      <th>STATE_AK</th>\n","      <th>STATE_AL</th>\n","      <th>STATE_AR</th>\n","      <th>STATE_AZ</th>\n","      <th>STATE_CA</th>\n","      <th>STATE_CO</th>\n","      <th>STATE_CT</th>\n","      <th>STATE_DC</th>\n","      <th>STATE_DE</th>\n","      <th>STATE_FL</th>\n","      <th>STATE_GA</th>\n","      <th>STATE_HI</th>\n","      <th>STATE_IA</th>\n","      <th>STATE_ID</th>\n","      <th>STATE_IL</th>\n","      <th>STATE_IN</th>\n","      <th>STATE_KS</th>\n","      <th>STATE_KY</th>\n","      <th>STATE_LA</th>\n","      <th>STATE_MD</th>\n","      <th>STATE_ME</th>\n","      <th>STATE_MI</th>\n","      <th>STATE_MN</th>\n","      <th>STATE_MO</th>\n","      <th>STATE_MS</th>\n","      <th>STATE_MT</th>\n","      <th>STATE_NC</th>\n","      <th>STATE_ND</th>\n","      <th>STATE_NE</th>\n","      <th>STATE_NH</th>\n","      <th>STATE_NM</th>\n","      <th>STATE_NV</th>\n","      <th>STATE_OH</th>\n","      <th>STATE_OK</th>\n","      <th>STATE_OR</th>\n","      <th>STATE_PA</th>\n","      <th>STATE_RI</th>\n","      <th>STATE_SC</th>\n","      <th>STATE_SD</th>\n","      <th>STATE_TN</th>\n","      <th>STATE_TX</th>\n","      <th>STATE_UT</th>\n","      <th>STATE_VA</th>\n","      <th>STATE_WA</th>\n","      <th>STATE_WI</th>\n","      <th>STATE_WV</th>\n","      <th>STATE_WY</th>\n","      <th>CARRIER_NAME_CARRIER1</th>\n","      <th>CARRIER_NAME_CARRIER10</th>\n","      <th>CARRIER_NAME_CARRIER11</th>\n","      <th>CARRIER_NAME_CARRIER12</th>\n","      <th>CARRIER_NAME_CARRIER13</th>\n","      <th>CARRIER_NAME_CARRIER14</th>\n","      <th>CARRIER_NAME_CARRIER2</th>\n","      <th>CARRIER_NAME_CARRIER3</th>\n","      <th>CARRIER_NAME_CARRIER4</th>\n","      <th>CARRIER_NAME_CARRIER5</th>\n","      <th>CARRIER_NAME_CARRIER6</th>\n","      <th>CARRIER_NAME_CARRIER7</th>\n","      <th>CARRIER_NAME_CARRIER8</th>\n","      <th>CARRIER_NAME_CARRIER9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>3.022930e+05</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","      <td>302293.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>1.016862e+08</td>\n","      <td>6.472092</td>\n","      <td>1.495281</td>\n","      <td>5.979887</td>\n","      <td>0.004188</td>\n","      <td>0.009398</td>\n","      <td>0.005240</td>\n","      <td>0.035026</td>\n","      <td>0.077111</td>\n","      <td>0.036862</td>\n","      <td>0.011343</td>\n","      <td>0.007900</td>\n","      <td>0.003348</td>\n","      <td>0.071791</td>\n","      <td>0.052294</td>\n","      <td>0.002838</td>\n","      <td>0.008740</td>\n","      <td>0.006308</td>\n","      <td>0.051447</td>\n","      <td>0.023325</td>\n","      <td>0.012021</td>\n","      <td>0.012762</td>\n","      <td>0.012521</td>\n","      <td>0.032472</td>\n","      <td>0.004343</td>\n","      <td>0.023762</td>\n","      <td>0.021989</td>\n","      <td>0.021304</td>\n","      <td>0.006960</td>\n","      <td>0.006990</td>\n","      <td>0.040729</td>\n","      <td>0.002028</td>\n","      <td>0.007311</td>\n","      <td>0.003292</td>\n","      <td>0.003804</td>\n","      <td>0.013834</td>\n","      <td>0.039425</td>\n","      <td>0.010301</td>\n","      <td>0.023742</td>\n","      <td>0.041579</td>\n","      <td>0.000572</td>\n","      <td>0.016137</td>\n","      <td>0.002709</td>\n","      <td>0.019551</td>\n","      <td>0.109056</td>\n","      <td>0.011816</td>\n","      <td>0.045714</td>\n","      <td>0.016742</td>\n","      <td>0.023209</td>\n","      <td>0.003156</td>\n","      <td>0.003010</td>\n","      <td>0.315968</td>\n","      <td>0.007116</td>\n","      <td>0.001674</td>\n","      <td>0.000278</td>\n","      <td>0.000245</td>\n","      <td>0.000132</td>\n","      <td>0.237379</td>\n","      <td>0.215003</td>\n","      <td>0.080435</td>\n","      <td>0.056905</td>\n","      <td>0.051364</td>\n","      <td>0.016696</td>\n","      <td>0.009550</td>\n","      <td>0.007255</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>8.565315e+06</td>\n","      <td>3.464492</td>\n","      <td>1.014413</td>\n","      <td>2.670176</td>\n","      <td>0.064579</td>\n","      <td>0.096488</td>\n","      <td>0.072198</td>\n","      <td>0.183845</td>\n","      <td>0.266767</td>\n","      <td>0.188422</td>\n","      <td>0.105899</td>\n","      <td>0.088528</td>\n","      <td>0.057763</td>\n","      <td>0.258142</td>\n","      <td>0.222619</td>\n","      <td>0.053200</td>\n","      <td>0.093078</td>\n","      <td>0.079175</td>\n","      <td>0.220908</td>\n","      <td>0.150934</td>\n","      <td>0.108982</td>\n","      <td>0.112248</td>\n","      <td>0.111195</td>\n","      <td>0.177250</td>\n","      <td>0.065762</td>\n","      <td>0.152306</td>\n","      <td>0.146646</td>\n","      <td>0.144395</td>\n","      <td>0.083137</td>\n","      <td>0.083313</td>\n","      <td>0.197661</td>\n","      <td>0.044986</td>\n","      <td>0.085190</td>\n","      <td>0.057277</td>\n","      <td>0.061561</td>\n","      <td>0.116803</td>\n","      <td>0.194605</td>\n","      <td>0.100971</td>\n","      <td>0.152244</td>\n","      <td>0.199625</td>\n","      <td>0.023916</td>\n","      <td>0.126001</td>\n","      <td>0.051980</td>\n","      <td>0.138450</td>\n","      <td>0.311711</td>\n","      <td>0.108059</td>\n","      <td>0.208864</td>\n","      <td>0.128304</td>\n","      <td>0.150568</td>\n","      <td>0.056089</td>\n","      <td>0.054784</td>\n","      <td>0.464901</td>\n","      <td>0.084054</td>\n","      <td>0.040879</td>\n","      <td>0.016667</td>\n","      <td>0.015644</td>\n","      <td>0.011502</td>\n","      <td>0.425477</td>\n","      <td>0.410825</td>\n","      <td>0.271966</td>\n","      <td>0.231661</td>\n","      <td>0.220740</td>\n","      <td>0.128129</td>\n","      <td>0.097258</td>\n","      <td>0.084864</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>2.791784e+07</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>9.417540e+07</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1.020674e+08</td>\n","      <td>7.000000</td>\n","      <td>1.000000</td>\n","      <td>6.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>1.088936e+08</td>\n","      <td>9.000000</td>\n","      <td>2.000000</td>\n","      <td>6.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.210835e+08</td>\n","      <td>12.000000</td>\n","      <td>10.000000</td>\n","      <td>12.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       POLICY_NUMBER    START_MONTH        MEMBERS  MAX_BILLING_CYCLE  \\\n","count   3.022930e+05  302293.000000  302293.000000      302293.000000   \n","mean    1.016862e+08       6.472092       1.495281           5.979887   \n","std     8.565315e+06       3.464492       1.014413           2.670176   \n","min     2.791784e+07       1.000000       1.000000           1.000000   \n","25%     9.417540e+07       3.000000       1.000000           3.000000   \n","50%     1.020674e+08       7.000000       1.000000           6.000000   \n","75%     1.088936e+08       9.000000       2.000000           6.000000   \n","max     1.210835e+08      12.000000      10.000000          12.000000   \n","\n","            STATE_AK       STATE_AL       STATE_AR       STATE_AZ  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.004188       0.009398       0.005240       0.035026   \n","std         0.064579       0.096488       0.072198       0.183845   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_CA       STATE_CO       STATE_CT       STATE_DC  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.077111       0.036862       0.011343       0.007900   \n","std         0.266767       0.188422       0.105899       0.088528   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_DE       STATE_FL       STATE_GA       STATE_HI  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.003348       0.071791       0.052294       0.002838   \n","std         0.057763       0.258142       0.222619       0.053200   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_IA       STATE_ID       STATE_IL       STATE_IN  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.008740       0.006308       0.051447       0.023325   \n","std         0.093078       0.079175       0.220908       0.150934   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_KS       STATE_KY       STATE_LA       STATE_MD  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.012021       0.012762       0.012521       0.032472   \n","std         0.108982       0.112248       0.111195       0.177250   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_ME       STATE_MI       STATE_MN       STATE_MO  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.004343       0.023762       0.021989       0.021304   \n","std         0.065762       0.152306       0.146646       0.144395   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_MS       STATE_MT       STATE_NC       STATE_ND  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.006960       0.006990       0.040729       0.002028   \n","std         0.083137       0.083313       0.197661       0.044986   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_NE       STATE_NH       STATE_NM       STATE_NV  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.007311       0.003292       0.003804       0.013834   \n","std         0.085190       0.057277       0.061561       0.116803   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_OH       STATE_OK       STATE_OR       STATE_PA  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.039425       0.010301       0.023742       0.041579   \n","std         0.194605       0.100971       0.152244       0.199625   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_RI       STATE_SC       STATE_SD       STATE_TN  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.000572       0.016137       0.002709       0.019551   \n","std         0.023916       0.126001       0.051980       0.138450   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_TX       STATE_UT       STATE_VA       STATE_WA  \\\n","count  302293.000000  302293.000000  302293.000000  302293.000000   \n","mean        0.109056       0.011816       0.045714       0.016742   \n","std         0.311711       0.108059       0.208864       0.128304   \n","min         0.000000       0.000000       0.000000       0.000000   \n","25%         0.000000       0.000000       0.000000       0.000000   \n","50%         0.000000       0.000000       0.000000       0.000000   \n","75%         0.000000       0.000000       0.000000       0.000000   \n","max         1.000000       1.000000       1.000000       1.000000   \n","\n","            STATE_WI       STATE_WV       STATE_WY  CARRIER_NAME_CARRIER1  \\\n","count  302293.000000  302293.000000  302293.000000          302293.000000   \n","mean        0.023209       0.003156       0.003010               0.315968   \n","std         0.150568       0.056089       0.054784               0.464901   \n","min         0.000000       0.000000       0.000000               0.000000   \n","25%         0.000000       0.000000       0.000000               0.000000   \n","50%         0.000000       0.000000       0.000000               0.000000   \n","75%         0.000000       0.000000       0.000000               1.000000   \n","max         1.000000       1.000000       1.000000               1.000000   \n","\n","       CARRIER_NAME_CARRIER10  CARRIER_NAME_CARRIER11  CARRIER_NAME_CARRIER12  \\\n","count           302293.000000           302293.000000           302293.000000   \n","mean                 0.007116                0.001674                0.000278   \n","std                  0.084054                0.040879                0.016667   \n","min                  0.000000                0.000000                0.000000   \n","25%                  0.000000                0.000000                0.000000   \n","50%                  0.000000                0.000000                0.000000   \n","75%                  0.000000                0.000000                0.000000   \n","max                  1.000000                1.000000                1.000000   \n","\n","       CARRIER_NAME_CARRIER13  CARRIER_NAME_CARRIER14  CARRIER_NAME_CARRIER2  \\\n","count           302293.000000           302293.000000          302293.000000   \n","mean                 0.000245                0.000132               0.237379   \n","std                  0.015644                0.011502               0.425477   \n","min                  0.000000                0.000000               0.000000   \n","25%                  0.000000                0.000000               0.000000   \n","50%                  0.000000                0.000000               0.000000   \n","75%                  0.000000                0.000000               0.000000   \n","max                  1.000000                1.000000               1.000000   \n","\n","       CARRIER_NAME_CARRIER3  CARRIER_NAME_CARRIER4  CARRIER_NAME_CARRIER5  \\\n","count          302293.000000          302293.000000          302293.000000   \n","mean                0.215003               0.080435               0.056905   \n","std                 0.410825               0.271966               0.231661   \n","min                 0.000000               0.000000               0.000000   \n","25%                 0.000000               0.000000               0.000000   \n","50%                 0.000000               0.000000               0.000000   \n","75%                 0.000000               0.000000               0.000000   \n","max                 1.000000               1.000000               1.000000   \n","\n","       CARRIER_NAME_CARRIER6  CARRIER_NAME_CARRIER7  CARRIER_NAME_CARRIER8  \\\n","count          302293.000000          302293.000000          302293.000000   \n","mean                0.051364               0.016696               0.009550   \n","std                 0.220740               0.128129               0.097258   \n","min                 0.000000               0.000000               0.000000   \n","25%                 0.000000               0.000000               0.000000   \n","50%                 0.000000               0.000000               0.000000   \n","75%                 0.000000               0.000000               0.000000   \n","max                 1.000000               1.000000               1.000000   \n","\n","       CARRIER_NAME_CARRIER9  \n","count          302293.000000  \n","mean                0.007255  \n","std                 0.084864  \n","min                 0.000000  \n","25%                 0.000000  \n","50%                 0.000000  \n","75%                 0.000000  \n","max                 1.000000  "]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Sjpyspuer9R3","colab_type":"code","colab":{}},"source":["# Extract the training data\n","train_data =ltv_df_model[ltv_df_model.START_DATE<\"2019-01-01\"]\n","test_set_2019 = ltv_df_model[ltv_df_model.START_DATE>=\"2019-01-01\"]\n","test_target_2019=test_set_2019['MAX_BILLING_CYCLE']\n","test_features_2019=test_set_2019.drop(columns = ['MAX_BILLING_CYCLE','POLICY_NUMBER','START_DATE'])\n","\n","# Labels for training\n","target =train_data['MAX_BILLING_CYCLE']\n","features=train_data.drop(columns = ['MAX_BILLING_CYCLE','POLICY_NUMBER','START_DATE'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCfxADdAvsBp","colab_type":"code","outputId":"efb3bba9-5ddb-4d26-8bf0-0a28cbbc7ce7","executionInfo":{"status":"ok","timestamp":1585510378852,"user_tz":420,"elapsed":113527,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Import train_test_split\n","from sklearn.model_selection  import train_test_split\n","\n","# Split the 'features' and 'income' data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(features, \n","                                                    target, \n","                                                    test_size = 0.2, \n","                                                    random_state = 0)\n","\n","# Show the results of the split\n","print (\"Training set has {} samples.\".format(X_train.shape[0]))\n","print (\"Testing set has {} samples.\".format(X_test.shape[0]))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Training set has 210675 samples.\n","Testing set has 52669 samples.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tPdDN9SKxAKO","colab_type":"code","colab":{}},"source":["from sklearn.metrics import fbeta_score,accuracy_score,f1_score\n","from sklearn.metrics import confusion_matrix\n","import time as t\n","\n","def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n","    '''\n","    inputs:\n","       - learner: the learning algorithm to be trained and predicted on\n","       - sample_size: the size of samples (number) to be drawn from training set\n","       - X_train: features training set\n","       - y_train: income training set\n","       - X_test: features testing set\n","       - y_test: income testing set\n","    '''\n","    \n","    results = {}\n","    \n","    start = t.time() # Get start time\n","    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n","    end = t.time() # Get end time\n","    \n","    # TODO: Calculate the training time\n","    results['train_time'] = end - start\n","    \n","        \n","    # TODO: Get the predictions on the test set(X_test),\n","    start = t.time() # Get start time\n","    predictions_test = learner.predict(X_test)\n","    predictions_train = learner.predict(X_train)\n","    end = t.time() # Get end time\n","   \n","    results['pred_time'] = end - start\n","            \n","    results['acc_train'] = accuracy_score(y_train,predictions_train)\n","   \n","    results['acc_test'] = accuracy_score(y_test,predictions_test)\n","    \n","    results['f_train'] = fbeta_score(y_train,predictions_train,beta=.5,average='weighted')\n","       \n","    results['f_test'] = fbeta_score(y_test,predictions_test,beta=.5,average='weighted')\n","    \n","    #labels = [0,1]\n","    #cm_train = confusion_matrix(y_train, predictions_train, labels)\n","    #cm_test = confusion_matrix(y_test, predictions_test, labels)\n","    display(learner)\n","    #display(cm_train)\n","    #display(cm_test)\n","       \n","    # Success\n","    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n","        \n","    # Return the results\n","    return results"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VR7K1ecES74d","colab_type":"text"},"source":["### 7c. Preliminary Comparison"]},{"cell_type":"code","metadata":{"id":"7ccDUxwvxQqd","colab_type":"code","outputId":"f75cde78-c5b0-4bc3-87b5-515cdf414004","executionInfo":{"status":"ok","timestamp":1585511666477,"user_tz":420,"elapsed":1400826,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","import lightgbm as lgb\n","\n","#clf_A = DecisionTreeClassifier(max_depth=6,min_samples_leaf =40)\n","clf_A = GradientBoostingClassifier()\n","clf_B = KNeighborsClassifier()\n","clf_C= RandomForestClassifier()\n","clf_D= XGBClassifier()\n","#clf_E=\n","\n","\n","\n","# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n","# HINT: samples_100 is the entire training set i.e. len(y_train)\n","# HINT: samples_10 is 10% of samples_100\n","# HINT: samples_1 is 1% of samples_100\n","samples_100 = len(y_train)\n","samples_10 = (samples_100//10)\n","samples_1 = samples_100//100\n","\n","\n","# Collect results on the learners\n","results = {}\n","for clf in [clf_A, clf_B, clf_C,clf_D]:\n","    clf_name = clf.__class__.__name__\n","    results[clf_name] = {}\n","    #print(clf)\n","    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n","      results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\n","\n"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n","                           learning_rate=0.1, loss='deviance', max_depth=3,\n","                           max_features=None, max_leaf_nodes=None,\n","                           min_impurity_decrease=0.0, min_impurity_split=None,\n","                           min_samples_leaf=1, min_samples_split=2,\n","                           min_weight_fraction_leaf=0.0, n_estimators=100,\n","                           n_iter_no_change=None, presort='deprecated',\n","                           random_state=None, subsample=1.0, tol=0.0001,\n","                           validation_fraction=0.1, verbose=0,\n","                           warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["GradientBoostingClassifier trained on 2106 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n","                           learning_rate=0.1, loss='deviance', max_depth=3,\n","                           max_features=None, max_leaf_nodes=None,\n","                           min_impurity_decrease=0.0, min_impurity_split=None,\n","                           min_samples_leaf=1, min_samples_split=2,\n","                           min_weight_fraction_leaf=0.0, n_estimators=100,\n","                           n_iter_no_change=None, presort='deprecated',\n","                           random_state=None, subsample=1.0, tol=0.0001,\n","                           validation_fraction=0.1, verbose=0,\n","                           warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["GradientBoostingClassifier trained on 21067 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n","                           learning_rate=0.1, loss='deviance', max_depth=3,\n","                           max_features=None, max_leaf_nodes=None,\n","                           min_impurity_decrease=0.0, min_impurity_split=None,\n","                           min_samples_leaf=1, min_samples_split=2,\n","                           min_weight_fraction_leaf=0.0, n_estimators=100,\n","                           n_iter_no_change=None, presort='deprecated',\n","                           random_state=None, subsample=1.0, tol=0.0001,\n","                           validation_fraction=0.1, verbose=0,\n","                           warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["GradientBoostingClassifier trained on 210675 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["KNeighborsClassifier trained on 2106 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["KNeighborsClassifier trained on 21067 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["KNeighborsClassifier trained on 210675 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["RandomForestClassifier trained on 2106 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["RandomForestClassifier trained on 21067 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n","                       criterion='gini', max_depth=None, max_features='auto',\n","                       max_leaf_nodes=None, max_samples=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["RandomForestClassifier trained on 210675 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=3,\n","              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n","              nthread=None, objective='multi:softprob', random_state=0,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["XGBClassifier trained on 2106 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=3,\n","              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n","              nthread=None, objective='multi:softprob', random_state=0,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["XGBClassifier trained on 21067 samples.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=3,\n","              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n","              nthread=None, objective='multi:softprob', random_state=0,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["XGBClassifier trained on 210675 samples.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5qRMiRnsTdrK","colab_type":"code","outputId":"2b0a4197-2e1a-42e1-b6f0-5317b0040599","executionInfo":{"status":"error","timestamp":1585511667496,"user_tz":420,"elapsed":1401705,"user":{"displayName":"Kanja Saha","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiHPvePGLi4Kg3WesMHvXMi-IGDS1PoNPTdxvnZtA=s64","userId":"03941799651680989401"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["compare_results=em.evaluate_metrics(results)\n","compare_results.compare_classification_metrics()"],"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-799c50c748bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcompare_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcompare_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_classification_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/ML_templates/utils/evaluate_metrics.py\u001b[0m in \u001b[0;36mcompare_classification_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;31m# Creative plot code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"10%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"100%\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABIkAAANSCAYAAADh7J46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 1440x1080 with 6 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"dyPsFTdaGZ0V","colab_type":"code","colab":{}},"source":["# Run metrics visualization for the three supervised learning models chosen\n","#evaluate(results)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YEk8SUxVS1o0","colab_type":"text"},"source":["### 7b. Model Training and Evaluation\n","\n","####1.   Random Forest Classifier\n","#####2.   XGB Classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"4kne8AP7zKkW","colab_type":"code","colab":{}},"source":["from sklearn.metrics import make_scorer\n","from sklearn.model_selection import GridSearchCV \n"," \n","clf= RandomForestClassifier()\n","\n","parameters = {'min_samples_leaf':[30,50,100,200,300], 'n_estimators':[100,500,1000,700],'max_features':[\"log2\",\"sqrt\",\"auto\"]}\n","\n","# TODO: Make an fbeta_score scoring object using make_scorer()\n","scorer = make_scorer(fbeta_score, beta=.5,average='weighted')\n","\n","# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n","grid_obj = GridSearchCV(clf,parameters,scorer)\n","\n","\n","# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n","grid_fit = grid_obj.fit(X_train,y_train)\n","\n","# Get the estimatorc\n","best_clf = grid_fit.best_estimator_\n","best_params=grid_fit.best_params_\n","\n","\n","# Make predictions using the unoptimized and the optimized model \n","l=clf.fit(X_train, y_train)\n","predictions = l.predict(X_test)\n","\n","#best_predictions = best_clf.predict(X_test)\n","\n","# Report the before-and-afterscores\n","print (\"Unoptimized model\\n------\")\n","print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n","print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5,average='weighted')))\n","print (\"\\nOptimized Model\\n------\")\n","print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n","print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,average='weighted')))\n","print (\"Best estimator parameters\")\n","print (best_params)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"92i5IJirfD6A","colab_type":"code","colab":{}},"source":["'''from xgboost import XGBClassifier\n","from sklearn.metrics import f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import make_scorer\n","from sklearn.model_selection import GridSearchCV \n","from sklearn.metrics import fbeta_score,accuracy_score,f1_score\n"," \n","clf= XGBClassifier()\n","\n","parameters = {'learning_rate':[.05,.1,.2,.3], 'max_depth':[6,7,8,9], 'min_child_weight':[1,2], \n","              'gamma':[0], 'subsample':[1], 'colsample_bytree':[.3,.4,.5], 'n_estimators':[100,150]\n","             , 'reg_lambda':[1]}\n","\n","# TODO: Make an fbeta_score scoring object using make_scorer()\n","scorer = make_scorer(fbeta_score, beta=.5,average='weighted')\n","\n","# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n","grid_obj = GridSearchCV(clf,parameters,scorer)\n","\n","\n","# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n","grid_fit = grid_obj.fit(X_train,y_train)\n","\n","# Get the estimatorc\n","best_clf = grid_fit.best_estimator_\n","best_params=grid_fit.best_params_\n","\n","\n","# Make predictions using the unoptimized and the optimized model \n","l=clf.fit(X_train, y_train)\n","predictions = l.predict(X_test)\n","#best_predictions = best_clf.predict(x_test)\n","\n","# Report the before-and-afterscores\n","print (\"Unoptimized model\\n------\")\n","print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n","print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5,average='weighted')))\n","print (\"\\nOptimized Model\\n------\")\n","print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n","print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,average='weighted')))\n","print (\"Best estimator parameters\")\n","print (best_params)'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNxrU_SCeM-J","colab_type":"code","colab":{}},"source":["'''import xgboost as xgb\n","def objective(params):\n","    params = {\n","        'max_depth': int(params['max_depth']),\n","        'gamma': \"{:.3f}\".format(params['gamma']),\n","        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n","    }\n","    \n","    clf = xgb.XGBClassifier(\n","        n_estimators=250,\n","        learning_rate=0.05,\n","        n_jobs=4,\n","        **params\n","    )\n","    \n","    score = cross_val_score(clf, X_train_d, Y_train_d, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n","    print(\"Gini {:.3f} params {}\".format(score, params))\n","    return score\n","\n","space = {\n","    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n","    'gamma': hp.uniform('gamma', 0.0, 0.5),\n","}\n","\n","trials = Trials()\n","best = fmin(fn=objective, space=space,algo=tpe.suggest,   max_evals=10,trials=trials)\n","best_model = getBestModelfromTrials(trials)'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oUmyzhaeUnW","colab_type":"code","colab":{}},"source":["'''import lightgbm as lgb\n","def objective(params):\n","    params = {\n","        'num_leaves': int(params['num_leaves']),\n","        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n","    }\n","    \n","    clf = lgbm.LGBMRegressor(\n","        n_estimators=500,\n","        learning_rate=0.01,\n","        **params\n","    )\n","    \n","    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n","    print(\"Gini {:.3f} params {}\".format(score, params))\n","    return score\n","\n","space = {\n","    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n","    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n","}\n","\n","\n","trials = Trials()\n","best = fmin(fn=objective, space=space,algo=tpe.suggest,   max_evals=10,trials=trials)\n","best_model = getBestModelfromTrials(trials)\n","print(\"Hyperopt estimated optimum {}\".format(best))\n","\n","print(\"Hyperopt estimated optimum {}\".format(best))'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEE_F1AKSinO","colab_type":"text"},"source":["### 8. Predicted Cumulative Retention Curve 2019"]},{"cell_type":"code","metadata":{"id":"GHGz0mh8hchD","colab_type":"code","colab":{}},"source":["predictions = l.predict(test_features_2019)\n","#best_predictions = best_clf.predict(x_test)\n","\n","# Report the before-and-afterscores\n","print (\"Unoptimized model\\n------\")\n","print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(test_target_2019, predictions)))\n","print (\"F-score on testing data: {:.4f}\".format(fbeta_score(test_target_2019, predictions, beta = 0.5,average='weighted')))\n","test_features_2019['PREDICTED_MAX_BILLING_CYCLE']=predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWvXIDi87fuf","colab_type":"code","colab":{}},"source":["best_predictions = best_clf.predict(test_features_2019)\n","print (\"\\nOptimized Model\\n------\")\n","print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(test_target_2019, best_predictions)))\n","print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,average='weighted')))\n","test_features_2019['PREDICTED_MAX_BILLING_CYCLE']=best_predictions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"txD3CNQJ-Jmb","colab_type":"code","colab":{}},"source":["def billing_cycle(members,cycle,max_billing_cycle):\n","    if cycle <= max_billing_cycle:\n","        return members\n","    else:\n","        return 0\n","\n","test_features_2019['START_DATE'] = test_set_2019['START_DATE']\n","test_features_2019['PREDICTED_BILLING_CYCLE_1'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,1, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_2'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,2, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_3'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,3, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_4'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,4, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_5'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,5, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_6'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,6, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_7'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,7, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_8'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,8, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_9'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,9, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_10'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,10, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_11'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,11, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019['PREDICTED_BILLING_CYCLE_12'] = test_features_2019.apply(lambda x: billing_cycle(x.MEMBERS,12, x.PREDICTED_MAX_BILLING_CYCLE), axis=1)\n","test_features_2019.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4_MmjW2_xN5","colab_type":"code","colab":{}},"source":["def aggfunc(x):\n","    data = {'MRR_CYCLE_1':100.0,\n","            'MRR_CYCLE_2': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_2)))/(sum(x.PREDICTED_BILLING_CYCLE_1)),\n","            'MRR_CYCLE_3': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_3)))/(sum(x.PREDICTED_BILLING_CYCLE_2)),\n","            'MRR_CYCLE_4': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_4)))/(sum(x.PREDICTED_BILLING_CYCLE_3)),\n","            'MRR_CYCLE_5': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_5)))/(sum(x.PREDICTED_BILLING_CYCLE_4)),\n","            'MRR_CYCLE_6': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_6)))/(sum(x.PREDICTED_BILLING_CYCLE_5)),\n","            'MRR_CYCLE_7': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_7)))/(sum(x.PREDICTED_BILLING_CYCLE_6)),\n","            'MRR_CYCLE_8': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_8)))/(sum(x.PREDICTED_BILLING_CYCLE_7)),\n","            'MRR_CYCLE_9': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_9)))/(sum(x.PREDICTED_BILLING_CYCLE_8)),\n","            'MRR_CYCLE_10': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_10)))/(sum(x.PREDICTED_BILLING_CYCLE_9)),\n","            'MRR_CYCLE_11': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_11)))/(sum(x.PREDICTED_BILLING_CYCLE_10)),\n","            'MRR_CYCLE_12': (100.0*(sum(x.PREDICTED_BILLING_CYCLE_12)))/(sum(x.PREDICTED_BILLING_CYCLE_11))}\n","\n","    return pd.Series(data)\n","\n","d = test_features_2019.groupby('START_DATE').apply(aggfunc).reset_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtltLSSuAJ9H","colab_type":"code","colab":{}},"source":["pd.options.mode.chained_assignment = None\n","d['CMRR_CYCLE_1']=100.0\n","d['CMRR_CYCLE_2']=(d['CMRR_CYCLE_1']*d['MRR_CYCLE_2'])/100\n","d['CMRR_CYCLE_3']=(d['CMRR_CYCLE_2']*d['MRR_CYCLE_3'])/100\n","d['CMRR_CYCLE_4']=(d['CMRR_CYCLE_3']*d['MRR_CYCLE_4'])/100\n","d['CMRR_CYCLE_5']=(d['CMRR_CYCLE_4']*d['MRR_CYCLE_5'])/100\n","d['CMRR_CYCLE_6']=(d['CMRR_CYCLE_5']*d['MRR_CYCLE_6'])/100\n","d['CMRR_CYCLE_7']=(d['CMRR_CYCLE_6']*d['MRR_CYCLE_7'])/100\n","d['CMRR_CYCLE_8']=(d['CMRR_CYCLE_7']*d['MRR_CYCLE_8'])/100\n","d['CMRR_CYCLE_9']=(d['CMRR_CYCLE_8']*d['MRR_CYCLE_9'])/100\n","d['CMRR_CYCLE_10']=(d['CMRR_CYCLE_9']*d['MRR_CYCLE_10'])/100\n","d['CMRR_CYCLE_11']=(d['CMRR_CYCLE_10']*d['MRR_CYCLE_11'])/100\n","d['CMRR_CYCLE_12']=(d['CMRR_CYCLE_11']*d['MRR_CYCLE_12'])/100\n","\n","\n","cumulative=d[['START_DATE','CMRR_CYCLE_1','CMRR_CYCLE_2','CMRR_CYCLE_3','CMRR_CYCLE_4','CMRR_CYCLE_5','CMRR_CYCLE_6','CMRR_CYCLE_7','CMRR_CYCLE_8','CMRR_CYCLE_9','CMRR_CYCLE_10','CMRR_CYCLE_11','CMRR_CYCLE_12']]\n","cumulative"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzZNb_1WAdR2","colab_type":"code","colab":{}},"source":["cumulative_melt=pd.melt(cumulative,id_vars=\"START_DATE\",var_name='BILLING_CYCLE',value_name='PREDICTED_CUMULATIVE_RETENTION_RATE')\n","cumulative_melt['BILLING_CYCLE']=cumulative_melt['BILLING_CYCLE'].str[11:].astype(int)\n","\n","timeseries=\"BILLING_CYCLE\"\n","x=\"COHORT\"\n","value=\"PREDICTED_CUMULATIVE_RETENTION_RATE\"\n","#df_d=df_group.groupby([timeseries,x], as_index=False)[value].agg({'group_size':'sum'})\n","plt.figure(figsize=(20,10))\n","sns.lineplot(x=\"BILLING_CYCLE\", y=\"PREDICTED_CUMULATIVE_RETENTION_RATE\", hue=\"START_DATE\",\n","                  data=cumulative_melt)\n","plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n","plt.title(\"Predicted Cumulative Retention Rate by \" + timeseries + \" and \" + x)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLsuYiYIfKrX","colab_type":"code","colab":{}},"source":["#HYPEROPT FOR LGBM\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","from timeit import default_timer as timer\n","import lightgbm as lgb\n","\n","MAX_EVALS = 50\n","N_FOLDS = 10\n","ITERATION = 0\n","# Define the search space\n","space = {\n","    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n","    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n","                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n","                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n","    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n","    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n","    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n","    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n","    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n","    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n","    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n","}\n","\n","\n","def objective(params, n_folds = N_FOLDS):\n","    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n","    print ('Params testing: ', params)\n","    # Keep track of evals\n","    global ITERATION\n","    \n","    ITERATION += 1\n","    \n","    # Retrieve the subsample if present otherwise set to 1.0\n","    subsample = params['boosting_type'].get('subsample', 1.0)\n","    \n","    # Extract the boosting type\n","    params['boosting_type'] = params['boosting_type']['boosting_type']\n","    params['subsample'] = subsample\n","    \n","    # Make sure parameters that need to be integers are integers\n","    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n","        params[parameter_name] = int(params[parameter_name])\n","    \n","    start = timer()\n","    \n","    # Perform n_folds cross validation\n","    train_set = lgb.Dataset(X_train, y_train)\n","    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n","                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n","    \n","    run_time = timer() - start\n","    \n","    # Extract the best score\n","    best_score = np.max(cv_results['auc-mean'])\n","    \n","    # Loss must be minimized\n","    loss = 1 - best_score\n","    \n","    # Boosting rounds that returned the highest cv score\n","    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n","\n","    # Write to the csv file ('a' means append)\n","    of_connection = open(out_file, 'a')\n","    writer = csv.writer(of_connection)\n","    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n","    \n","    pred_auc =model.predict_proba(X_test, batch_size = 128, verbose = 0)\n","    acc = roc_auc_score(y_test, pred_auc)\n","    print('AUC:', acc)\n","    sys.stdout.flush() \n","   \n","    # Dictionary with information for evaluation\n","    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n","            'estimators': n_estimators, \n","            'train_time': run_time, 'status': STATUS_OK}\n","\n","bayes_trials = Trials()\n","best = fmin(fn=objective,  space=space,  algo=tpe.suggest, max_evals=MAX_EVALS,trials = bayes_trials)\n","\n","print(\"Hyperopt estimated optimum {}\".format(best))"],"execution_count":0,"outputs":[]}]}